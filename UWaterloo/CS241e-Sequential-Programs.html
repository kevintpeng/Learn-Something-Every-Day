<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <meta name="generator" content="pandoc">
  
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="/style.css" type="text/css">
          <title>Sequential Programs</title>
</head>
<body>

<h1 id="sequential-programs">Sequential Programs</h1>          <a href="index.html">Back to UWaterloo</a>
<div id="TOC">

<ul>
<li><a href="#mips-aside">MIPS [aside]</a></li>
<li><a href="#abstraction-labels-variables">Abstraction: labels &amp; variables</a></li>
<li><a href="#relocation-and-linking">Relocation and Linking</a></li>
<li><a href="#evaluating-expressions">Evaluating Expressions</a></li>
<li><a href="#static-link">Static Link</a></li>
<li><a href="#first-class-functions">First class functions</a></li>
<li><a href="#heap">Heap</a></li>
<li><a href="#a6-tutorial">A6 Tutorial</a></li>
<li><a href="#finite-automata">Finite Automata</a></li>
<li><a href="#scanning">Scanning</a></li>
<li><a href="#regular-expressions">Regular Expressions</a></li>
<li><a href="#context-free-grammars">Context-Free Grammars</a></li>
<li><a href="#parsing">Parsing</a></li>
<li><a href="#cyk">CYK</a></li>
<li><a href="#earleys-algorithm">Earley's Algorithm</a></li>
<li><a href="#context-sensitive-analysis">Context Sensitive Analysis</a></li>
<li><a href="#memory-management">Memory Management</a></li>
<li><a href="#garbage-collector">Garbage Collector</a></li>
<li><a href="#cheneys-algorithm">Cheney's Algorithm</a></li>
<li><a href="#lambda-calculus">Lambda Calculus</a></li>
<li><a href="#church-encoding">Church Encoding</a></li>
<li><a href="#exam">Exam</a></li>
</ul>
</div>
<h3 id="mips-aside">MIPS [aside]</h3>
<ul>
<li>a reduced instruction set computer (RISC) instruction set architecture (ISA)</li>
<li>MIPS implementations are primarily used in embedded systems</li>
<li>tries to avoid the use of stacks (because of speed)<ul>
<li>MIPS reserves four registers to pass arguments <code>$a0</code> ... 3</li>
<li>passing arguments that cannot be contained in 4 registers use the stack</li>
<li>however, nested functions still require the use of the stack</li>
</ul>
</li>
<li>this kind of architecture is advantageous for leaf procedures, where there are no nested function calls</li>
</ul>
<h3 id="abstraction-labels-variables">Abstraction: labels &amp; variables</h3>
<ul>
<li>an <strong>opcode</strong> is a word that represents a machine language instruction (ADD, JR ... )</li>
<li>
<strong>assembly language</strong> is a lang for writing machine lang programs using opcodes instead of bits</li>
<li>
<strong>assembler</strong> is a program that translates assembly lang. into machine lang.</li>
<li>
<strong>label</strong> is an abstraction of a memory address (think variable assignment)</li>
<li>instructions are words (strings of bits)</li>
<li>a <strong>code</strong> can be a word for instructions or extra features like labels, that are not instructions</li>
<li>
<p>to eliminate labels, first pass build a symbol table, a map from labels to addresses. second pass converts each use of label to address/offset</p>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash"><span class="ex">Find</span> abs value of <span class="va">$1</span> (two<span class="st">'s complement)</span>
<span class="st">SLT(2,1,0)</span>
<span class="st">beq(2, 0, label) # beq takes labels while BEQ takes offsets</span>
<span class="st">SUB(1, 0, 1)</span>
<span class="st">Define(label)</span>
<span class="st">JR(31) # end the program</span></code></pre></div>
</li>
</ul>
<h3 id="relocation-and-linking">Relocation and Linking</h3>
<ul>
<li>def: an <em>object file</em> contains machine language code with additional metadata, which records how labels were used before translation to machine lang (which addresses need to be offset)</li>
<li>def: <em>relocation</em> is the process of adjusting addresses in machine lang code so it can be loaded at different memory addresses<ul><li>conceptually, it reverse engineering what the assembly code would have been, then it reassembled at some offset, loaded at a different memory address</li></ul>
</li>
<li>def: <em>linking</em> is the process of combining object files into one (object file, program, library)<ul><li>to link, relocate conflicting object files and labels based on metadata</li></ul>
</li>
<li>Typical c compilation process: <code>.c</code> compiles to assembly <code>.s</code>, assemble to object files <code>.o</code> , then link object files to an executable program</li>
</ul>
<p>Stack, <code>$30</code> stores the stack pointer, pointing to the top of the stack</p>
<ul>
<li>to access variables, you need an offsest from the stack pointer</li>
<li>a symbol table maps variables to offsets</li>
<li><p>a <em>frame pointer</em> at <code>$29</code> is a copy of the stack pointer made at the beginning of a procedure that stays constant for the <em>duration of the procedure call</em>, this is used as the origin of offset for accessing variables in the current stack frame. This allows the actual stack pointer to change without ruining our symbol table</p></li>
<li>
<em>variables</em> are abstractions of storage locations that hold a value</li>
<li>the <em>extent</em> or lifetime of a variable instance is the time interval in which the variable can be accessed<ul>
<li>local vars have the extent of execution of procedure, global have extent of entire program, fields of objects have extent of time until object is freed (explicitly or by GC)</li>
<li>variables are stored at offests relative to the frame pointer</li>
</ul>
</li>
<li>Symbol table holds the offset of each variable<ul>
<li>translation time/compile time is when you tkae high level program with variables and translate to machine code. Symbol table is used here to replace variables with their respective address</li>
<li>at compile time, to access variable, <code>LW(3,8,29)</code> loads into R3, from an offset of 8 to the frame pointer</li>
</ul>
</li>
<li>on the stack, allocate chunk of consecutive memory locations rather than single bytes. Each chunk has 2 reserved words of memory: first is size, second is reserved by convention<ul><li>in assignments, allocate all memory in chunks</li></ul>
</li>
</ul>
<h3 id="evaluating-expressions">Evaluating Expressions</h3>
<ol><li>Using the stack:</li></ol>
<ul>
<li>for <code>(e1 op e2)</code>, build a binary expression tree, generate code to evaluate <code>e1</code> and <code>e2</code>
</li>
<li>store intermediate responses on the stack</li>
<li>
<p>conceptually simple, but verbose</p>
<pre><code>evaluate(e1, op, e2)
evaluate(e1)
push $3 onto stack
evaluate(e2)
pop from stack $4
$3 = $4 pop $3</code></pre>
</li>
</ul>
<ol><li>Using temp vars/virtual registers:</li></ol>
<ul>
<li>
<p>flexible but still generates lots of variables unnecessarily</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="fu">evaluate</span>(e1, op, e2):(Code, Variable) = {
<span class="kw">val</span> (c1, v1) = <span class="fu">evaluate</span>(e1)
<span class="kw">val</span> (c1, v1) = <span class="fu">evaluate</span>(e2)
<span class="kw">val</span> result = <span class="kw">new</span> Variable
<span class="kw">val</span> code = <span class="fu">block</span>(c1,c2,<span class="st">"result = v1 op v2"</span>)
(code, result)
}</code></pre></div>
</li>
<li>
<strong>register allocation</strong> is the process of assigning virtual register (Variables) to real registers or memory locations<ul>
<li>minimize number of real registers/stack locations by sharing them among non-interfering virtual registers</li>
<li>intermediate representation with virtual registers =&gt; register allocation =&gt; intermediate representation with real registers and stack locations</li>
</ul>
</li>
</ul>
<ol><li>Using temp vars with operations on registers</li></ol>
<ul><li>
<p>same as first, but adds abstraction with lends to more efficient underlying implementation</p>
<pre><code>evaluate(e1 op e2) {
withTempVar {
  block(
   evaluate(e1)
   write(t1,$3)
   evaluate(e2)
   read($4, t1)
   $3 = $4 op $3
  )
}
}</code></pre>
</li></ul>
<p>If statements:</p>
<pre><code>if (e1 op e2) T else E

evaluate(e1)
t1 = $3
evaluate(e2)
$4 = t1
comparison of $4 and $3, branch to labelElse
T
jump to labelEnd
Define(labelElse)
  E
Define(labelEnd)</code></pre>
<ul>
<li>a variable is <strong>live at program point p</strong> if the value that it holds is called (even if just conditionally) read sometime after P</li>
<li>the <strong>live range</strong> of a variable is the set of program points where it is live</li>
<li>two variables can share the same register if their live ranges do not overlap</li>
<li>the start of a live range is always just after a write to the var</li>
<li>the end of a live range is always just before a read to the var</li>
<li>
<strong>interference graph</strong> has each variable as a vertex and edges between variables that share a live range<ul>
<li>apply graph colouring</li>
<li>finding a minimal colouring (fewest colours) is NP-Hard</li>
<li>simple greedy algorithm: for each vertex, assign the smallest colour not used by its neighbours</li>
<li>some special structured graphs allow for efficient algorithms</li>
</ul>
</li>
<li>Procedures: an abstraction that encapsulates a resuable sequence of code<ul>
<li>calling code transfers control to procedure (modifies PC), passing arguments for parameters</li>
<li>procedure-transfer control back to caller<ul><li>returns a value</li></ul>
</li>
<li>caller and callee must agree on conventions:<ul>
<li>which registers to pass arguments and return values</li>
<li>which registers procedures may modify or preserve</li>
<li>register with return address (31 or Reg.savedPC)</li>
<li>who allocates/frees memory</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash"><span class="co"># CALLER</span>
<span class="ex">Stack.allocate</span>(parameters)
<span class="ex">LIS</span>(Reg.targetPC)
<span class="ex">Use</span>(proc)
<span class="ex">JALR</span>(Reg.targetPC)

<span class="co"># PROCEDURE/CALLEE</span>
<span class="ex">Define</span>(proc)
<span class="ex">Reg.savedParamPointer</span> = Reg.allocated  
<span class="ex">Stack.allocate</span>(frame)
<span class="ex">dynamicLink</span> = Reg.framePointer  <span class="co"># saved frame pointer of caller</span>
<span class="co"># savedPC is on the Stack, this is needed for nested procedures</span>
<span class="ex">savedPC</span> = Reg.savedPC <span class="co"># Reg 31</span>
<span class="ex">Reg.framePointer</span> = Reg.allocated
<span class="ex">paramPointer</span> = Reg.savedParamPointer
<span class="co"># body</span>
<span class="ex">Reg.savedPC</span> = savedPC
<span class="ex">Reg.framePointer</span> = dynamicLink
<span class="ex">Stack.pop</span> <span class="co"># pops frame chunk</span>
<span class="ex">Stack.pop</span> <span class="co"># pops paramaters chunk</span>
<span class="ex">JR</span>(Reg.savedPC) </code></pre></div>
<p>On the stack:</p>
<ul>
<li>size</li>
<li>Assignment11</li>
<li>variables</li>
<li>savedPC</li>
<li>dynamicLink</li>
<li>paramPointer<br>
argChunk (pointed to by paramPointer):</li>
<li>size</li>
<li>A11</li>
<li>arguments<br>
Some Conventions:</li>
<li>Caller-save (what registers will be written to): 31 savedPC, 3 result</li>
<li>Callee-save: 29 framePointer, 30 stackPointer</li>
</ul>
<p>a5 - don't mess up Reg.result</p>
<h3 id="static-link">Static Link</h3>
<p>Static lib<br>
A static link is a pointer to the frame of the statically enclosed procedure of the currently executing procedure</p>
<ul>
<li>the nesting depth of a proc is the number of outer procs that it is nested inside of</li>
<li>top-level depth is 0</li>
<li>to access variable (eliminateVarAccessesA6), n = depth(current proc) - depth(proc declaring variable)<ul>
<li>follow static link n times, then find variable in that frame</li>
<li>to compute static link at call site:<ul>
<li>we want depth(static link) = depth(call target) - 1</li>
<li>let n = depth(current proc) - depth(static link)</li>
<li>n = depth(current proc) - depth(call target) + 1</li>
<li>if n is 0, pass parameter pointer for static link</li>
<li>else follow static link n times and pass to callee as static link. n ≥ 0</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="first-class-functions">First class functions</h3>
<ul>
<li>should treat functions as any other object</li>
<li>
<p>related to lambda's, anonymous functions and closures</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> <span class="fu">procedure</span>(x: Int) = { x + <span class="dv">1</span> } 
<span class="kw">var</span> increase: ((Int) =&gt; Int) = procedure
increase = { x + <span class="dv">2</span> } <span class="co">// nameless, so { x + 2 } is an anonymous function</span>
increase = { x + increment } <span class="co">// increment is undefined, it's a FREE VARIABLE</span>
<span class="fu">increaseBy</span>(increment: Int) : (Int) =&gt; Int = {
{ x =&gt; x + increment }
} <span class="co">// increaseBy closes increment</span></code></pre></div>
</li>
<li>a free variable in an expression is a variable that is not defined within that expression<ul>
<li>x is not free in { x =&gt; x + increment } above, but increment is free</li>
<li>an expression is closed if it has no free variables</li>
</ul>
</li>
<li>a closure is an expression with its environment for the free variable in the expression body<ul><li>implements a function value</li></ul>
</li>
<li>in this example, the environment is the frame of increaseBy. The closure closes over the environment<ul><li>the closure adds the environment for the lambda { x =&gt; x + increment } bu defining what increment really is.</li></ul>
</li>
</ul>
<h3 id="heap">Heap</h3>
<p>A data structure that manages memory so it can be allocated and freed</p>
<ul>
<li>A6 - allocate and never free</li>
<li>A11 - allocate and free when extent ends</li>
<li>every procedure with a closure nested inside will have a frame on the heap</li>
</ul>
<h4 id="objects">Objects</h4>
<p>Objects can be thought of in terms of closures</p>
<ul>
<li>objects have state and behaviour</li>
<li>a collection of closures (indexed by names) that close over the same environment</li>
</ul>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> newCounter: (()=&gt;<span class="dt">int</span>, (<span class="dt">int</span>)=&gt;Unit) = {
  <span class="kw">var</span> value = <span class="dv">0</span>
  <span class="kw">def</span> <span class="fu">get</span>() = value
  <span class="kw">def</span> <span class="fu">incrementBy</span>(amount: Int) = {
    value = value + amount
  }
  (get, incrementBy) <span class="co">// returns a pair of functions, that share an environment stored on the HEAP</span>
}</code></pre></div>
<h3 id="a6-tutorial">A6 Tutorial</h3>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// Testing your closures locally</span>

<span class="kw">def</span> <span class="fu">increaseBy</span>(increment: Int): (Int)=&gt; Int = {
  <span class="kw">def</span> <span class="fu">procedure</span>(x:Int) = { x + increment }
  procedure
}

<span class="kw">def</span> <span class="fu">main</span>(a: Int, b: Int) = (<span class="fu">increaseBy</span>(a))(b)

<span class="co">// now reimplemented in Lacs</span>
<span class="kw">val</span> increment = <span class="kw">new</span> <span class="fu">Variable</span>(<span class="st">"increment"</span>)
<span class="kw">val</span> increaseby = <span class="kw">new</span> <span class="fu">Procedure</span>(<span class="st">"increaseBy"</span>,
  Seq(increment), Seq())

<span class="kw">val</span> x = <span class="kw">new</span> <span class="fu">Variable</span>(<span class="st">"x"</span>)

<span class="co">// 4th arg, Option can be None or some other Procedure </span>
<span class="kw">val</span> procedure = <span class="kw">new</span> <span class="fu">Procedure</span>(<span class="st">"procedure"</span>, Seq(x), Seq(), Some(increaseBy))

<span class="co">// some helper function</span>
<span class="kw">def</span> <span class="fu">v</span>(variable: Variable): Code = <span class="fu">read</span>(Reg.<span class="fu">result</span>, variable)
<span class="co">// now define the body of the procedure</span>
<span class="co">// make sure you implement some way that accesses outer scoped variable (should be similar to a regular read)</span>
procedure.<span class="fu">code</span> = <span class="fu">binOp</span>(<span class="fu">v</span>(x), plus, <span class="fu">v</span>(increment)) <span class="co">// increment is an outer variable</span>

increaseBy.<span class="fu">code</span> = <span class="fu">Closure</span>(procedure)

<span class="kw">val</span> a = <span class="kw">new</span> <span class="fu">Variable</span>(<span class="st">"a"</span>)
<span class="kw">val</span> b = <span class="kw">new</span> <span class="fu">Variable</span>(<span class="st">"b"</span>)

<span class="kw">val</span> parameterVar = <span class="kw">new</span> <span class="fu">Variable</span>(<span class="st">"parameterVar"</span>)
<span class="kw">val</span> main = <span class="kw">new</span> <span class="fu">Procedure</span>(<span class="st">"main"</span>, Seq(a,b), Seq())

<span class="co">// you can't use paramChunk because you don't know until runtime which paramChunk to access outer vars</span>
main.<span class="fu">code</span> = <span class="fu">CallClosure</span>(<span class="fu">call</span>(increaseBy, <span class="fu">v</span>(a)), b, Seq(parameterVar))

<span class="kw">val</span> machineCode = <span class="fu">compilerA6</span>(Seq(main, increaseBy, procedure))
<span class="kw">val</span> endState = A1.<span class="fu">loadAndRun</span>(machineCode.<span class="fu">words</span>, <span class="fu">Word</span>(<span class="fu">encodeSigned</span>(<span class="dv">1</span>)), <span class="fu">Word</span>(<span class="fu">encodeSigned</span>(<span class="dv">2</span>)))</code></pre></div>
<ul>
<li>For compilerA6, why do we need two phases? Varaccesses handles outer variable. To do that, you need the frames of all the outer procedures. We don't compute the frame of a procedure until runtime.</li>
<li>Phase one, translate all procedures and generate frames. Then EliminateVarAccesses phase 2.</li>
<li>phaseOneResults is a map from procedure to frame</li>
<li>eliminateCalls -&gt; closurecalls, static links?</li>
<li>implementCall: Closure calls &amp; Regular Calls. ImplementCall is the similarity between the two.</li>
</ul>
<h3 id="finite-automata">Finite Automata</h3>
<p>In Theory of Computation, a Deterministic Finite Automata is a deterministic finite state machine that accepts and rejects finite strings of symbols, to produce a unique run of the automata for each input string. It recognizes exactly the set of regular languages, useful for lexical analysis and pattern matching</p>
<p>[ASIDE] A regular language is a formal language (set of string and symbols constrained by rules) that can be expressed with regular expressions.</p>
<ul>
<li>the collection of regular languages over an alphabet ∑ is defined recursively<ul>
<li>The empty language ø and the empty string language {ε} are regular languages</li>
<li>forall a in ∑, the singleton language {a} is a regular language</li>
<li>if A and B are regular languages, then A u B, A•B (concat), and A* (Kleene Star, one or more) are regular languages</li>
<li>no other languages over ∑ are regular</li>
</ul>
</li>
<li>it is a language accepted by deterministic and nondeterministic finite automaton</li>
</ul>
<p>Formally, a DFA is a 5-tuple; &lt;∑, Q, q0, A, ∂&gt; where</p>
<ul>
<li>∑ is a finite alphabet</li>
<li>Q is a finite set of states</li>
<li>q0 is an element of Q, is the starting state</li>
<li>A is a subset of Q, is the accepting states; machine reports the input string is a member of the language it accepts</li>
<li>∂: Q x ∑ -&gt; Q is a transition function</li>
</ul>
<p>DFA recognition algorithm:</p>
<ul>
<li>define ∂* : Q x ∑* -&gt; Q extended transition function, ∑* is a string of symbols from ∑</li>
<li>∂*(q, e) = q</li>
<li><p>∂* (q, head::tail) = ∂* ( ∂(q,head),tail), recursively applies alphabet symbols with function ∂() to move states</p></li>
<li>A word is <strong>accepted by the DFA</strong> if ∂* (q0, w) is an element of A</li>
<li>the language defined/specified by a DFA is the set of words accepted by the DFA</li>
<li><p>a language is <strong>regular</strong> if exists a DFA that specifies it</p></li>
</ul>
<p>Non-determininistic Finite Automata (NFA) has multiple choices of transitions between states</p>
<ul>
<li>a word is accepted by a NFA if any path leads to accepting state</li>
<li>the machine takes all choices at once, so the NFA is a set of states at any given point, rather than a single state</li>
</ul>
<p>Formally, a NFA is a 5-tuple; &lt;∑, Q, q0, A, ∂&gt; same as DFA except <code>∂: Q x ∑ -&gt; 2^Q</code> is a transition function to set of states, where 2^Q is the <a href="../Math-CS/Sets.md#set-of-all-subsets">power set</a></p>
<p>NFA recognition algorithm:</p>
<ul>
<li>define <code>∂* : Q x ∑* -&gt; 2^Q</code> extended transition function, <code>∑*</code> is a string of symbols from ∑</li>
<li><code>∂*(q, e) = {q}</code></li>
<li><code>∂*(q, head::tail) = union of all( q' in ∂(q,head) . ∂*(q', tail))</code></li>
</ul>
<p>A word <code>w</code> is accepted by an NFA if <code>∂*(q0, w) n A ≠ {}</code>, the set of possible transitions is contained in <code>A</code>, the set of accepting states</p>
<p>NFA-ε, accepts ε (empty string) as a possible string input. It nicely represents unknown states. To eliminate all epsilon-transitions from an NFA:</p>
<ul><li>where an epsilon transfer (<code>∂*(q, epsilon) = {q}</code>) is a given element of the alphabet that, for example, causes state A -&gt; B, then replace all transitions to A with a transition to B, and remove the epsilon-transition from A to B</li></ul>
<h3 id="scanning">Scanning</h3>
<p>Breaking a string into tokens using an NFA constrained by maximal munch property</p>
<ul>
<li>Recognition: is some word in language?</li>
<li>Scanning: split a string into tokens s.t. each token is in language</li>
<li>maximal munch is the principle that when creating some construct, as much of the available input should be consumed</li>
<li>input: string w, language L</li>
<li>output: sequence of words w1, w2 ... wn s.t.<ul>
<li>w1, w2...wn = w</li>
<li>forall i . wi in L</li>
<li>forall i, wi is the longest prefix of wi, wi+1 ... wn that is in L (maximal munch)</li>
</ul>
</li>
<li><em>when tokenizing, break the string into the largest tokens possible</em></li>
</ul>
<p>Let L be language of tokens, <code>L*</code> be the language of words that can be scanned</p>
<ul>
<li>Given an NFA for L, construct an NFA for <code>L*</code> by adding epsilon-transitions from every accepting state to start state</li>
<li>since NFA is nondeterministic, we constrain each transition to use the maximal munch to produce a unique transition<ul>
<li>:( might not find a solution when one exists because it chooses branches of maximum token size and over looks a possible path to a solution</li>
<li>if <code>L = {aa, aaa}</code>, <code>W = aaaa</code>, then maximal munch first takes the token <code>aaa</code> from W, leaving W = <code>a</code>. <code>a</code> is not in L, so our method says no solution, even though it's possible to take the tokens <code>aa</code> and <code>aa</code> from W.</li>
</ul>
</li>
</ul>
<p>Method:</p>
<ol>
<li>run DFA for L until it gets stuck</li>
<li>if in non-accepting state, <strong>backtrack</strong> to last-visited accepting state. if no accepting states visited, ERROR</li>
<li>output a token corresponding to accepting state</li>
<li>set DFA to start state and repeat from 1 (this is why we have to add epsilon transitions for <code>L*</code> eariler)</li>
</ol>
<h3 id="regular-expressions">Regular Expressions</h3>
<p>A regular expression expresses a regular language. So L is basically a function that maps regular expressions to the sets of strings that it represents.</p>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash">ε       <span class="ex">L</span>(ε) = <span class="dt">{ε}</span> <span class="co"># empty string</span>
<span class="ex">a</span> in ∑  L(a) = <span class="dt">{a}</span> <span class="co"># literal character</span>
ø       <span class="ex">L</span>(ø) = <span class="dt">{}</span> <span class="co"># empty set</span>
<span class="ex">R1</span> <span class="kw">|</span> <span class="ex">R2</span> L(R1<span class="kw">|</span><span class="ex">R2</span>) = <span class="ex">L</span>(R1) <span class="ex">u</span> (L(R2) <span class="co"># R1 &amp; R2 are regex</span>
<span class="ex">R1R2</span>    L(R1R2) = {<span class="ex">xy</span> <span class="kw">|</span> <span class="ex">x</span> in L(R1), <span class="ex">y</span> in L(R2)} <span class="co"># concatenation of elements contained within each of the regular languages</span>
<span class="ex">R*</span>      L(R*) = <span class="kw">{</span> <span class="ex">x1x2..xn</span> <span class="kw">|</span> <span class="ex">forall</span> i . xi in L(R), <span class="ex">n</span> ≥ 0 <span class="kw">}</span> <span class="co"># Kleene's Star</span></code></pre></div>
<p>Kleene's Theorem:</p>
<ul><li>For a language L, the following statements are equivalent to each other:</li></ul>
<ol>
<li>exists a DFA specifying L</li>
<li>exists an NFA without epsilon-transitions specifying L</li>
<li>exists an NFA specifying L (could have epsilon)</li>
<li>exists a regex specifying L</li>
<li>L is regular</li>
</ol>
<h3 id="context-free-grammars">Context-Free Grammars</h3>
<p>Regular expressions can't express unlimited recursive structures. We want nesting in programming languages, so we need something more powerful than regular languages.</p>
<p>A context-free grammar (CFG) is a set of recursive rewriting rules (productions) used to generate patterns of strings. Regular Expressions use iteration, Context-free Grammars use recursion</p>
<ul>
<li>set of terminal symbols ∑, which are the characters of the alphabet that appear in the strings</li>
<li>a set of nonterminal symbols N, placeholders for patterns of terminal symbols that can be generated by nonterminal symbols</li>
<li>a set of productions P, rules for replacing nonterminal symbols on the left side of the production with other nonterminal or terminal symbols on the right</li>
<li>a start symbol S, which is a special nonterminal symbol that appears in the initial string generated by the grammar</li>
<li>A CFG may have a production for a nonterminal to the empty string (epsilon), effectively removing the nonterminal from the generated string</li>
</ul>
<p><em>You can represent all regular languages with CFGs, but not all context-free languages can be described by regular expressions/dfas/nfas</em></p>
<p>To generate a string of terminal symbols from a CFG:</p>
<ul>
<li>start with a string consisiting of the start symbol</li>
<li>apply one of the productions with the start symbol</li>
<li>repeat the process of selecting nonterminal symbols in the string and replacing them with the right side of corresponding productions until only terminal symbols remain</li>
<li>αAβ <strong>directly derives</strong> (in one step) αγβ if A → γ ∈ P</li>
<li><p>a <strong>derives</strong> (in 0 or more steps) an if a1 =&gt; a2 =&gt; ... =&gt; an (a =&gt;* an)</p></li>
<li>Def: A formal grammar is a set of production rules for strings in a formal language</li>
<li>Def: A regular grammar is a formal grammar that is right-regular or left-regular. For A, B, C in N, a in ∑:<ul>
<li>a right regular grammar where B -&gt; a, B -&gt; Ca, or B -&gt; epsilon</li>
<li>a left regular grammar is where A -&gt; a, A -&gt; aB, or A -&gt; epsilon</li>
</ul>
</li>
<li><em>a regular grammar describes a language describable by a regular expression. DFAs, Regular Grammar and Regular Expressions are just three formalizations of the same concept.</em></li>
<li><p>context free languages are generated by CFGs, the set of all Regular languages are a proper subset of all Context Free Languages</p></li>
</ul>
<p>canonical derivations: rightmost and leftmost, for precedence of parsing</p>
<ul>
<li>there exists a parse tree iff exists rightmost derivation iff exists leftmost derivation</li>
<li>A CFG is <strong>ambiguous</strong> if it allows multiple parse trees for the same input (different paths of production rules for the same output)</li>
</ul>
<h3 id="parsing">Parsing</h3>
<ul>
<li>Top-down parsing starts with S and finds derivation steps leading to result w. The hard problem is choosing the right rules</li>
<li>Bottom-up algorithm starts with w, finds derivation steps leading to s.</li>
<li>Chomsky Normal Form: RHS of CFG can have an arbitrary number of symbols</li>
<li>in CNF form, production rules can only have either two nonterminals on the RHS or one terminal</li>
<li>formally, CFGs are allowed to have empty productions NP -&gt; epsilon, which can always be eliminated without changing the language via epsilon (by altering production rules)</li>
</ul>
<h3 id="cyk">CYK</h3>
<p>CYK starts with S, starting nonterminal, as alpha. We want to expand nonterminals in alpha such that the input string is represented by the CFG. So, check all possible chains of production rules until it finds a valid parse tree. It follows four cases, each trying to reduce the string of terminal and non-terminal characters, alpha, to the base case where alpha is empty. Since alpha is our current state, we keep checking the leftmost character from alpha to see if it can reduce our input, with the end goal being an empty alpha and empty input (i.e. all of the input is represented by a parse tree of terminal/nonterminal characters). Complexity occurs when the leading character of alpha is a nonterminal, since you have to now have to split the input string in two, such that the nonterminal describes the first partition and the rest of alpha describes the second partition of the input string. So there are a quadratic number of substrings of input that could be checked, each with a linear time partition check. So O(n^3) time complexity. Dynamic programming is used to optimize the problem by eliminating repeated calculations with a memoization table.<br>
- Bottom up parsing, with dynamic programming to save results in table<br>
- given w, does <code>s =&gt;* w</code><br>
- sub-problem: given x, does <code>alpha =&gt;* x</code>, alpha is a sequence of terminals and nonterminals, x is a seq of characters<br>
- w =yxz for some y,z. x is a substring of w<br>
- A -&gt; gamma alpha in P. alpha is a suffix of RHS of a rule in R<br>
- <em>so we have to test for every suffix alpha and every substring x, in O(w^3) time, since for each w^2 substrings, check against O(w) partitions of each substring.</em><br>
- the table contains single entries for each nonterminal, where each cell of the table has a substring<br>
- cases for filling our table:<br>
- alpha -&gt; epsilon (empty string) : true if x = epsilon<br>
- alpha -&gt; aß : true if x = ay AND <code>ß =&gt;* y</code><br>
- alpha -&gt; A : true if exists A -&gt; gamma in P s.t. <code>gamma =&gt;* x</code><br>
- alpha -&gt; Aß : if exists y,z where x=y and <code>A =&gt;* y</code> and <code>ß=&gt;* z</code></p>
<h3 id="earleys-algorithm">Earley's Algorithm</h3>
<ul>
<li>bottom-up</li>
<li>P(w^2) Space</li>
<li>O(w^3) time for ambiguous</li>
<li>O(w^2) time for unambiguous</li>
</ul>
<h3 id="context-sensitive-analysis">Context Sensitive Analysis</h3>
<p>Reject programs that satisfy a grammar but are still invalid</p>
<ul><li>in LACs, types prevent accidential misinterpretation</li></ul>
<p>Lacs Types</p>
<ul>
<li>Int<ul><li>integers between -2<sup>31</sup> to 2<sup>31</sup>-1 with arithmetic modulo 2<sup>32</sup>
</li></ul>
</li>
<li>(type, ...) =&gt; type<ul><li>functions that take arguments of the specified types, return a value of the specified type</li></ul>
</li>
</ul>
<p>A type is a collection of values. It is an interpretation of a sequence of bits</p>
<ul>
<li>a type is a compatable property of programs that guarantee some property of their execution</li>
<li>ensures consistent use of operations</li>
<li>identify how much memory is needed for a value</li>
</ul>
<p>A <strong>type system</strong> is a set of rules htat compute the type of expression from the types of its subexpressions</p>
<ul><li>type system is <strong>sound</strong> if when it computes a type gamma for expression e, then e evaluates to value v in γ</li></ul>
<p>Using premise-conclusion notation from proofs, type inference rules:</p>
<pre><code>Γ |- NUM: Int    in the context Γ (captial gamma), the symbol table, NUM has type Int 
Γ(ID) = γ        Γ must be fed through all rules so that our symbol table is accessible when needed
____________
Γ |- ID: γ

Let E in {expras, expra, expr, term, factor}

Γ |- E1 : Int
Γ |- E2 : Int
_____________
Γ |- E1 op E2 : Int   for op being arithmetic operation

</code></pre>
<p>Type tree has two passes: first pass get all "defdef"s and create new ProcedureScopes for them. Second pass, create a symbol table for each.</p>
<h3 id="memory-management">Memory Management</h3>
<p>Heap is a data structure to manage memory that can be allocated/freed at any time</p>
<ul><li>operations:<ul>
<li>new/malloc/allocate: allocate a new block of memory whose size can be determined at runtime</li>
<li>free/delete</li>
</ul>
</li></ul>
<p>The stack and heap grow toward each other, by our convention stack grows by subtraction, heap by addition.</p>
<p>We can structure our heap with blocks of memory, each block holding metadata about size and free/used. Then use a linked list of free blocks. A <strong>fragmented</strong> heap is split into small blocks.</p>
<h3 id="garbage-collector">Garbage Collector</h3>
<p>Figures out the extent of objects for you, frees them implicitly.</p>
<ul>
<li>allocating a frame on the heap may call the garbage collector, which will call a function that could overwrite savedParamPtr</li>
<li>after A6, the frame might be allocated on the heap. So always allocate on the stack first, then when you're done with Reg.savedPC, copy the chunk to the heap if needed</li>
<li>when you allocate, you can only use machine language because a lot has been eliminated by transformations. Write it with A6 tests-style</li>
</ul>
<h3 id="cheneys-algorithm">Cheney's Algorithm</h3>
<p>A stop and copy algorithm, which splits our total heap size in half, and performs collection by copying live objects (ones with pointers to it) from one semispace (the "from-space") to the other semispace (the "to-space")</p>
<ul><li>we refer to the semi-spaces as the "to" and "from" spaces rather than semispace 1 and 2 because their role swaps everytime we perform garbage collection</li></ul>
<p>The algorithm works in 2 parts:</p>
<ul>
<li>check object references from the stack, and do one of the two following:<ul>
<li>if it hasn't been copied yet, make a copy in the to-space, put a forwarding pointer in place of the object in the from-space and update the reference to point to the newly copied object</li>
<li>if it's been copied, then update the reference using the forwarding pointer</li>
</ul>
</li>
<li>then check the objects in the to-space for references and perform the above checks</li>
</ul>
<h3 id="lambda-calculus">Lambda Calculus</h3>
<p>For expressing computation based on function abstraction. It is turing complete and thus acts as a universal model of computation</p>
<ul>
<li>λ expressions and terms denote binding a variable to a function (think anonymous functions)</li>
<li>square_sum(x,y) = x^2 + y^2 can be written in anonymous form as (x,y) |-&gt; x^2 + y^2</li>
<li>all functions in lambda calculus can be represented as single input functions<ul>
<li>(x,y) |-&gt; x^2 + y^2 can be written as x |-&gt; (y |-&gt; x^2 + y^2)</li>
<li>this method is <strong>currying</strong>, chaining function calls together</li>
</ul>
</li>
<li>id(x) = x is the identity function, useful for having a value represented as an anonymous function</li>
</ul>
<p>lambda calculus consists of a language of <strong>lambda terms</strong>, defined by formal syntax and a set of transformation rules</p>
<ul>
<li>
<strong>lambda expressions</strong> can be valid or invalid, a valid expression is a term</li>
<li>all syntactically valid lambda terms are inductively defined by 3 rules:<ul>
<li>a variable x is a lambda term</li>
<li>
<strong>abstraction</strong>: if t is a lambda term, (λx.t) is a lambda term</li>
<li>
<strong>application</strong>: if t and s are a lambda terms, (ts) is a lambda term</li>
<li>nothing else is a lambda term</li>
</ul>
</li>
<li>an abstraction λx.t is a defn of an anonymous function that takes a single parameter x and substituting it into expression t<ul><li>abstraction binds free occurences of x in t</li></ul>
</li>
<li>application is like composition of terms; ts means t(s)</li>
<li>Β-reduction is performing expression substitution during application over an abstraction</li>
<li>α-conversion is renaming variables to avoid variable binding over an existing, same named variable<ul><li>has practical use for programming languages, where a variable is defined twice, once in a nested scope</li></ul>
</li>
</ul>
<h3 id="church-encoding">Church Encoding</h3>
<p>Means of representing data operators in lambda calculus (since in untyped lambda calculus, we only have function types)</p>
<ul>
<li>church numerals are a representation of the natural numbers, using lambda calculus</li>
<li>church encoding maps primatives using higher-order functions</li>
<li>any computable operator and operands can be represented under church encoding (though not actually practical in use)<ul><li>it demonstrates the completeness of untyped lambda calculus</li></ul>
</li>
</ul>
<p>Church numerals can be used to represent natural numbers by the number of times some function f is applied to x:</p>
<ul>
<li>0 = λf.λx.x</li>
<li>1 = λf.λx.f x</li>
<li>2 = λf.λx.f (f x)</li>
<li>n = λf.λx f<sup>n</sup> x</li>
<li>addition = λm.λn.λf.λx.f<sup>m</sup> (f<sup>n</sup> (x))</li>
<li>successor function succ(n) is Β-equivalent to (plus 1)</li>
</ul>
<h3 id="exam">Exam</h3>
<ul>
<li>Closures, tail calls</li>
<li>formal languages</li>
<li>regular languages</li>
<li>DFAs, NFAs, Regex</li>
<li>Scanning</li>
<li>CFGs<ul>
<li>parse trees</li>
<li>derivations</li>
<li>ambiguity</li>
</ul>
</li>
<li>CYK parsing</li>
<li>Other parsing algorithms (simple comparisons, big O)</li>
<li>name resolution</li>
<li>type-checking</li>
<li>heap</li>
<li>explicit malloc/free</li>
<li>fragmentation, compaction</li>
<li>cheney's garbage collector</li>
<li>lambda calculus</li>
</ul>
    <div id="footer">
      Notes by <a href="https://github.com/kevintpeng">Kevin Peng</a>, Google intern.<br>
      Formerly Riot Games, Bloomberg, Apple &amp; Shopify.<br>
      Connect with me on <a href="https://www.linkedin.com/in/kevintpeng/">LinkedIn</a>.
    </div>
</body>
</html>
