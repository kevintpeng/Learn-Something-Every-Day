<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <meta name="generator" content="pandoc">
  
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="/style.css" type="text/css">
          <title>End-to-End Memory Networks 2015</title>
</head>
<body>

<h1 id="end-to-end-memory-networks-2015"><a href="https://arxiv.org/pdf/1503.08895.pdf">End-to-End Memory Networks 2015</a></h1>          <a href="index.html">Back to Research-Papers</a>
<h1 id="datascience-machinelearning">datascience #machinelearning</h1>
<p>Recent trend in deep learning, using explicit storage and the notion of attention</p>
<ul>
<li>attention mechanisms in Neural Networks is loosely based on human visual attention<ul><li>focus on different regions of the image</li></ul>
</li>
<li>this paper focuses on architecture for RNNs, where the recurrence depends on reads from large external memory</li>
</ul>
    <div id="footer">
      Notes by <a href="https://github.com/kevintpeng">Kevin Peng</a>, Google intern.<br>
      Formerly Riot Games, Bloomberg, Apple &amp; Shopify.<br>
      Connect with me on <a href="https://www.linkedin.com/in/kevintpeng/">LinkedIn</a>.
    </div>
</body>
</html>
